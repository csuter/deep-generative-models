{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: find images that are adversarial for humans using a black-box attack, and see what happens when you show adversarial examples to machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Adversarial Examples that Fool both Computer Vision and Time-Limited Humans\n",
    "\n",
    "Some adversarial examples for neural networks fool resource-limited humans ([Elsayed et al. 2018](https://arxiv.org/abs/1802.08195)). You present an image to a human and ask them to categorize the image.\n",
    "\n",
    "![Experiment setup](w3-assets/experiment_setup.png)\n",
    "\n",
    "With this setup, they show that images that are adversarially trained to fool artificial neural networks can also fool humans. There are a few tricks to make the adversarial images work on humans:\n",
    "\n",
    "* the adversial images are found from an ensemble of networks (more robust examples)\n",
    "* backward masking makes the task harder so humans aren't near saturation\n",
    "* artifical retina concentrates the effect near fixation point where acuity is highest in humans\n",
    "    \n",
    "However, the effects are very small - 51-53% choices towards the adversarial category:\n",
    "\n",
    "<img src=\"w3-assets/class_probability.png\" width=\"250\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Deep Learning Models Resistant to Adversarial Attacks\n",
    "\n",
    "https://arxiv.org/pdf/1706.06083.pdf\n",
    "\n",
    "Important observation: classification loss and adversarial risk are two very different things that a classifier can be optimized for. \n",
    "\n",
    "*Classification loss*: the goal of classification is minimize the empirical risk (e.g. cross-entropy loss) over the data function:\n",
    "\n",
    "$$\\mathbb{E}_{(x,y)\\sim D} L(x,y,\\theta)$$\n",
    "\n",
    "Where $x$ are examplars, $y$ are labels, $L$ is the classification loss, $\\theta$ are the network parameters, and $D$ is the data distribution. \n",
    "\n",
    "On the other hand, the *adversarial risk* has to do with the likelihood of misclassifying a data point within a neighborhood $S$ of a data point $x$. $S$ might be the $L_\\infty$ ball, for example. So the first loss has to do with the data distribution, the other with points that are outside the data distribution.\n",
    "\n",
    "To combine these two notions, we try to find a set of parameters $\\theta$ that minimizes the risk:\n",
    "\n",
    "$$\\mathbb{E}_{(x,y)\\sim D} \\max_{\\delta \\in S} L(x+\\delta,y,\\theta)$$\n",
    "\n",
    "This has an interesting minimax form that is studied in the paper.\n",
    "\n",
    "*PGD*: Projected gradient descent. Do gradient descent with respect to the loss and project back into the $S$ neighborhood ball. See explanation here: https://math.stackexchange.com/a/572664/583479\n",
    "\n",
    "Section 3 fizzles with empirical investigations of the structure of the loss function, but makes an argument that PGD is the master first-order method. They make an argument that a robust set of weights is found by replacing examples with PGD based adversarial examples.\n",
    "\n",
    "Higher capacity models are more robust to adversarial attacks because they can learn more complicated boundaries.\n",
    "\n",
    "Iterated PGD is a best-in-class adversarial attack method for white box problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Risk and the Dangers of Evaluating Against Weak Attacks\n",
    "\n",
    "https://arxiv.org/pdf/1802.05666.pdf\n",
    "\n",
    "> *Abstract*: This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. The existence of adversarial examples in trained neural networks reflects the fact that expected risk alone does not capture the model's performance against worst-case inputs. We motivate the use of adversarial risk as an objective, although it cannot easily be computed exactly. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may be obscured to adversaries, by optimizing this surrogate rather than the true adversarial risk. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.\n",
    "\n",
    "> We note two potential explanations for models with high evaluation performance:  models which learn a robust decision boundary, and thus admit few adversarial examples, and models which achieve security via obscurity.\n",
    "\n",
    "Is it possible that humans have are not really robust, but hide their non-robustness via obscurity? What if we try to open up the box?\n",
    "\n",
    "Rather than finding an adversary according to the true cross-entropy, they use a surrogate loss:\n",
    "\n",
    "$$J(x) = \\text{logit}_i(x) - \\max_{j\\ne i}\\text{logit}_j(x)$$\n",
    "\n",
    "Here $i$ is the label of the real example $x_0$. This surrogate loss is easier to optimize than the cross-entropy, and has the right properties, e.g. it's negative when $x$ is an adversarial example.\n",
    "\n",
    "To perform black-box optimization, they use the SPSA algorithm, which works by randomly perturbing the input and obtaining an estimated gradient. One could construct a similar method where one would instead approximate the local function as a plane, take random samples on that plane, and solve for the maximum likelihood plane under a condition of small slope. Their algorithm looks like this:\n",
    "\n",
    "<img src=\"w3-assets/spsa_attack.png\" width=\"500\" />\n",
    "\n",
    "They also point out two other methods: natural evolutionary strategies and zero-order optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTA adversarial methods and gradient estimation methods\n",
    "\n",
    "* [Obfuscated gradients give a false sense of security](https://www.anishathalye.com/media/2018/07/19/poster.pdf): lots of ways of breaking an adversarial defense. If we think that humans have adversarial defenses, maybe we can break them with some of these techniques. Most of the techniques quoted are for white-box methods, i.e. methods where gradients are computable. However, there are many references in this poster that are also relevant to black box attacks.\n",
    "* [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/pdf/1706.06083.pdf): \n",
    "* [Gradient estimation](Adversarial Risk and the Dangers of Evaluating Against Weak Attacks):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
